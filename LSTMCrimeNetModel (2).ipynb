{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "QRFqSs0lsRJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the data, putting the different crimes in their own category, grouping by areaName, week start, and crm cd desc which is basically the type of crime commited"
      ],
      "metadata": {
        "id": "RlG2Xppqx3NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiU8A4xvj2mq",
        "outputId": "09e8814c-d9cf-4256-c4b5-f860c2367a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/HighPerformanceMachineLearning/CrimeDatafrom2020toPresent.csv')\n",
        "\n",
        "# Convert the date column to datetime\n",
        "df['DATE OCC'] = pd.to_datetime(df['DATE OCC'])\n",
        "\n",
        "# Create a new column for week start (Monday of the week)\n",
        "df['WEEK_START'] = df['DATE OCC'].dt.to_period('W').apply(lambda r: r.start_time)\n",
        "\n",
        "# -----------------------\n",
        "# 🔹 Group Crime Categories\n",
        "# -----------------------\n",
        "crime_category_map = {\n",
        "    # Burglary\n",
        "    'BURGLARY': 'Burglary',\n",
        "    'BURGLARY FROM VEHICLE': 'Burglary',\n",
        "    'BURGLARY, ATTEMPTED': 'Burglary',\n",
        "    'BURGLARY FROM VEHICLE, ATTEMPTED': 'Burglary',\n",
        "\n",
        "    # Theft\n",
        "    'SHOPLIFTING - PETTY THEFT ($950 & UNDER)': 'Theft',\n",
        "    'SHOPLIFTING - ATTEMPT': 'Theft',\n",
        "    'SHOPLIFTING-GRAND THEFT ($950.01 & OVER)': 'Theft',\n",
        "    'THEFT PLAIN - PETTY ($950 & UNDER)': 'Theft',\n",
        "    'THEFT, PERSON': 'Theft',\n",
        "    'PICKPOCKET': 'Theft',\n",
        "    'PETTY THEFT - AUTO REPAIR': 'Theft',\n",
        "    'BUNCO, PETTY THEFT': 'Theft',\n",
        "    'BUNCO, GRAND THEFT': 'Theft',\n",
        "    'EMBEZZLEMENT, GRAND THEFT ($950.01 & OVER)': 'Theft',\n",
        "    'EMBEZZLEMENT, PETTY THEFT ($950 & UNDER)': 'Theft',\n",
        "    'THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)': 'Theft',\n",
        "    'THEFT FROM MOTOR VEHICLE - GRAND ($950.01 AND OVER)': 'Theft',\n",
        "    'THEFT OF IDENTITY': 'Theft',\n",
        "    'THEFT PLAIN - ATTEMPT': 'Theft',\n",
        "\n",
        "    # Assault\n",
        "    'BATTERY - SIMPLE ASSAULT': 'Assault',\n",
        "    'BATTERY POLICE (SIMPLE)': 'Assault',\n",
        "    'ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT': 'Assault',\n",
        "    'INTIMATE PARTNER - AGGRAVATED ASSAULT': 'Assault',\n",
        "    'INTIMATE PARTNER - SIMPLE ASSAULT': 'Assault',\n",
        "\n",
        "    # Vehicle Theft\n",
        "    'VEHICLE - STOLEN': 'Vehicle Theft',\n",
        "    'VEHICLE - ATTEMPT STOLEN': 'Vehicle Theft',\n",
        "    'DRIVING WITHOUT OWNER CONSENT (DWOC)': 'Vehicle Theft',\n",
        "\n",
        "    # Sexual Offenses\n",
        "    'RAPE, FORCIBLE': 'Sexual Offense',\n",
        "    'RAPE, ATTEMPTED': 'Sexual Offense',\n",
        "    'ORAL COPULATION': 'Sexual Offense',\n",
        "    'SODOMY/SEXUAL CONTACT B/W PENIS OF ONE PERS TO ANUS OTH': 'Sexual Offense',\n",
        "    'SEXUAL PENETRATION W/FOREIGN OBJECT': 'Sexual Offense',\n",
        "    'LEWD/LASCIVIOUS ACTS WITH CHILD': 'Sexual Offense',\n",
        "    'SEX,UNLAWFUL(INC MUTUAL CONSENT, PENETRATION W/ FRGN OBJ': 'Sexual Offense',\n",
        "\n",
        "    # Homicide\n",
        "    'CRIMINAL HOMICIDE': 'Homicide',\n",
        "    'MANSLAUGHTER, NEGLIGENT': 'Homicide',\n",
        "\n",
        "    # Vandalism\n",
        "    'VANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS)': 'Vandalism',\n",
        "    'VANDALISM - MISDEAMEANOR ($399 OR UNDER)': 'Vandalism',\n",
        "\n",
        "    # Robbery\n",
        "    'ROBBERY': 'Robbery',\n",
        "    'ATTEMPTED ROBBERY': 'Robbery',\n",
        "\n",
        "    # Arson\n",
        "    'ARSON': 'Arson',\n",
        "\n",
        "    # Threats\n",
        "    'CRIMINAL THREATS - NO WEAPON DISPLAYED': 'Threats',\n",
        "    'THREATENING PHONE CALLS/LETTERS': 'Threats',\n",
        "\n",
        "    # Weapons\n",
        "    'WEAPONS POSSESSION/BOMBING': 'Weapons',\n",
        "    'BRANDISH WEAPON': 'Weapons',\n",
        "\n",
        "    # Fraud\n",
        "    'CREDIT CARDS, FRAUD USE ($950 & UNDER': 'Fraud',\n",
        "    'CREDIT CARDS, FRAUD USE ($950.01 & OVER)': 'Fraud',\n",
        "    'DOCUMENT FORGERY / STOLEN FELONY': 'Fraud',\n",
        "\n",
        "    # Other\n",
        "    # You can expand this section as needed\n",
        "}\n",
        "\n",
        "# Apply mapping\n",
        "df['Crime Category'] = df['Crm Cd Desc'].map(crime_category_map).fillna('Other')\n",
        "\n",
        "# -----------------------\n",
        "# 🔹 Weekly Grouped & Pivot\n",
        "# -----------------------\n",
        "weekly_grouped = (\n",
        "    df.groupby(['AREA NAME', 'WEEK_START', 'Crime Category'])\n",
        "    .size()\n",
        "    .reset_index(name='count')\n",
        ")\n",
        "\n",
        "weekly_pivot = weekly_grouped.pivot_table(\n",
        "    index=['AREA NAME', 'WEEK_START'],\n",
        "    columns='Crime Category',\n",
        "    values='count',\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "weekly_pivot['Total Crimes'] = weekly_pivot.sum(axis=1)\n",
        "\n",
        "weekly_pivot = weekly_pivot.reset_index()"
      ],
      "metadata": {
        "id": "z4reeB5ZwDlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating our Datasets class"
      ],
      "metadata": {
        "id": "0aIpDWjH1lG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CrimeSequenceDataset(Dataset):\n",
        "    def __init__(self, df, input_window=4):\n",
        "        self.input_window = input_window\n",
        "        self.areas = df['AREA NAME'].unique()\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        self.feature_cols = [col for col in df.columns if col not in ['AREA NAME', 'WEEK_START']]\n",
        "\n",
        "        # Build sequences per area\n",
        "        for area in self.areas:\n",
        "            area_df = df[df['AREA NAME'] == area].sort_values('WEEK_START')\n",
        "            feature_data = area_df[self.feature_cols].values\n",
        "\n",
        "            for i in range(len(feature_data) - input_window):\n",
        "                x_seq = feature_data[i:i + input_window]\n",
        "                y_seq = feature_data[i + input_window]\n",
        "\n",
        "                self.inputs.append(x_seq)\n",
        "                self.targets.append(np.log1p(y_seq))  # Apply log1p to target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.inputs[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "KHI8sVjfwhhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate the Dataset"
      ],
      "metadata": {
        "id": "kI3eVKSh4TBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace 'weekly_pivot' with you DataFrame if named differently\n",
        "input_window = 4\n",
        "dataset = CrimeSequenceDataset(weekly_pivot, input_window = input_window)"
      ],
      "metadata": {
        "id": "OKjQYjrC4Suw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating an LSTM model utilizing PyTorch"
      ],
      "metadata": {
        "id": "I0ClV0nb1qou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrimeLSTMModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
        "    super(CrimeLSTMModel, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #X shape: (batch_size, sequence_length, input_size)\n",
        "    out, _ = self.lstm(x) #Output shape: (batch, seq_len, hidden_size)\n",
        "    out = out[:, -1, :] #Get the last output in the sequence\n",
        "    out = self.fc(out) #Final prediction\n",
        "    return out"
      ],
      "metadata": {
        "id": "so2cZYRa1aDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA Training Loop in PyTorch\n",
        "This loop will:\n",
        "* Send inputs and targets to GPU (cuda)\n",
        "* Train using MSELoss\n",
        "* Track training loss over epochs\n"
      ],
      "metadata": {
        "id": "nyriPeff2qte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# === Setup ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"✅ Using device:\", device)\n",
        "\n",
        "# Auto-get input/output sizes from sample\n",
        "sample_x, _ = dataset[0]\n",
        "input_size = sample_x.shape[1]\n",
        "output_size = input_size\n",
        "hidden_size = 128\n",
        "batch_size = 64\n",
        "epochs = 300\n",
        "\n",
        "# === DataLoader ===\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "# === Loss Function ===\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# ================================\n",
        "# 🔁 Train Parallel Model\n",
        "# ================================\n",
        "import time\n",
        "\n",
        "# ================================\n",
        "# 🔁 Train Parallel Model with Timing\n",
        "# ================================\n",
        "print(\"\\n🔁 Training Parallel Model\")\n",
        "model_parallel = CrimeLSTMModel(input_size, hidden_size, output_size)\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"🚀 Using\", torch.cuda.device_count(), \"GPUs with DataParallel\")\n",
        "    model_parallel = nn.DataParallel(model_parallel)\n",
        "\n",
        "model_parallel = model_parallel.to(device)\n",
        "optimizer_parallel = optim.Adam(model_parallel.parameters(), lr=0.001)\n",
        "\n",
        "start_time = time.time()  # ⏱️ Start timing\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_parallel.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer_parallel.zero_grad()\n",
        "        outputs = model_parallel(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer_parallel.step()\n",
        "        running_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"[Parallel] Epoch {epoch+1}/{epochs} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()  # ⏱️ Stop timing\n",
        "print(f\"⏱️ Total training time for Parallel Model: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# ================================\n",
        "# 🔁 Train Non-Parallel Model with Timing\n",
        "# ================================\n",
        "print(\"\\n🔁 Training Non-Parallel Model\")\n",
        "model_sequential = CrimeLSTMModel(input_size, hidden_size, output_size).to(device)\n",
        "optimizer_seq = optim.Adam(model_sequential.parameters(), lr=0.001)\n",
        "\n",
        "start_time = time.time()  # ⏱️ Start timing\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_sequential.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer_seq.zero_grad()\n",
        "        outputs = model_sequential(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer_seq.step()\n",
        "        running_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"[Sequential] Epoch {epoch+1}/{epochs} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()  # ⏱️ Stop timing\n",
        "print(f\"⏱️ Total training time for Non-Parallel Model: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZNsOO_R2iKH",
        "outputId": "13cf1ecc-cd6f-467e-f417-b1a452669091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using device: cuda\n",
            "\n",
            "🔁 Training Parallel Model\n",
            "[Parallel] Epoch 1/300 Loss: 1.5727\n",
            "[Parallel] Epoch 2/300 Loss: 0.1575\n",
            "[Parallel] Epoch 3/300 Loss: 0.1485\n",
            "[Parallel] Epoch 4/300 Loss: 0.1453\n",
            "[Parallel] Epoch 5/300 Loss: 0.1438\n",
            "[Parallel] Epoch 6/300 Loss: 0.1423\n",
            "[Parallel] Epoch 7/300 Loss: 0.1414\n",
            "[Parallel] Epoch 8/300 Loss: 0.1410\n",
            "[Parallel] Epoch 9/300 Loss: 0.1401\n",
            "[Parallel] Epoch 10/300 Loss: 0.1394\n",
            "[Parallel] Epoch 11/300 Loss: 0.1391\n",
            "[Parallel] Epoch 12/300 Loss: 0.1385\n",
            "[Parallel] Epoch 13/300 Loss: 0.1379\n",
            "[Parallel] Epoch 14/300 Loss: 0.1375\n",
            "[Parallel] Epoch 15/300 Loss: 0.1375\n",
            "[Parallel] Epoch 16/300 Loss: 0.1373\n",
            "[Parallel] Epoch 17/300 Loss: 0.1365\n",
            "[Parallel] Epoch 18/300 Loss: 0.1368\n",
            "[Parallel] Epoch 19/300 Loss: 0.1362\n",
            "[Parallel] Epoch 20/300 Loss: 0.1362\n",
            "[Parallel] Epoch 21/300 Loss: 0.1357\n",
            "[Parallel] Epoch 22/300 Loss: 0.1353\n",
            "[Parallel] Epoch 23/300 Loss: 0.1356\n",
            "[Parallel] Epoch 24/300 Loss: 0.1356\n",
            "[Parallel] Epoch 25/300 Loss: 0.1358\n",
            "[Parallel] Epoch 26/300 Loss: 0.1357\n",
            "[Parallel] Epoch 27/300 Loss: 0.1349\n",
            "[Parallel] Epoch 28/300 Loss: 0.1347\n",
            "[Parallel] Epoch 29/300 Loss: 0.1346\n",
            "[Parallel] Epoch 30/300 Loss: 0.1345\n",
            "[Parallel] Epoch 31/300 Loss: 0.1345\n",
            "[Parallel] Epoch 32/300 Loss: 0.1338\n",
            "[Parallel] Epoch 33/300 Loss: 0.1343\n",
            "[Parallel] Epoch 34/300 Loss: 0.1337\n",
            "[Parallel] Epoch 35/300 Loss: 0.1339\n",
            "[Parallel] Epoch 36/300 Loss: 0.1337\n",
            "[Parallel] Epoch 37/300 Loss: 0.1331\n",
            "[Parallel] Epoch 38/300 Loss: 0.1339\n",
            "[Parallel] Epoch 39/300 Loss: 0.1333\n",
            "[Parallel] Epoch 40/300 Loss: 0.1332\n",
            "[Parallel] Epoch 41/300 Loss: 0.1328\n",
            "[Parallel] Epoch 42/300 Loss: 0.1327\n",
            "[Parallel] Epoch 43/300 Loss: 0.1328\n",
            "[Parallel] Epoch 44/300 Loss: 0.1322\n",
            "[Parallel] Epoch 45/300 Loss: 0.1322\n",
            "[Parallel] Epoch 46/300 Loss: 0.1324\n",
            "[Parallel] Epoch 47/300 Loss: 0.1323\n",
            "[Parallel] Epoch 48/300 Loss: 0.1319\n",
            "[Parallel] Epoch 49/300 Loss: 0.1321\n",
            "[Parallel] Epoch 50/300 Loss: 0.1316\n",
            "[Parallel] Epoch 51/300 Loss: 0.1320\n",
            "[Parallel] Epoch 52/300 Loss: 0.1318\n",
            "[Parallel] Epoch 53/300 Loss: 0.1320\n",
            "[Parallel] Epoch 54/300 Loss: 0.1316\n",
            "[Parallel] Epoch 55/300 Loss: 0.1315\n",
            "[Parallel] Epoch 56/300 Loss: 0.1309\n",
            "[Parallel] Epoch 57/300 Loss: 0.1309\n",
            "[Parallel] Epoch 58/300 Loss: 0.1309\n",
            "[Parallel] Epoch 59/300 Loss: 0.1307\n",
            "[Parallel] Epoch 60/300 Loss: 0.1306\n",
            "[Parallel] Epoch 61/300 Loss: 0.1304\n",
            "[Parallel] Epoch 62/300 Loss: 0.1304\n",
            "[Parallel] Epoch 63/300 Loss: 0.1297\n",
            "[Parallel] Epoch 64/300 Loss: 0.1300\n",
            "[Parallel] Epoch 65/300 Loss: 0.1301\n",
            "[Parallel] Epoch 66/300 Loss: 0.1298\n",
            "[Parallel] Epoch 67/300 Loss: 0.1299\n",
            "[Parallel] Epoch 68/300 Loss: 0.1297\n",
            "[Parallel] Epoch 69/300 Loss: 0.1299\n",
            "[Parallel] Epoch 70/300 Loss: 0.1292\n",
            "[Parallel] Epoch 71/300 Loss: 0.1293\n",
            "[Parallel] Epoch 72/300 Loss: 0.1292\n",
            "[Parallel] Epoch 73/300 Loss: 0.1296\n",
            "[Parallel] Epoch 74/300 Loss: 0.1291\n",
            "[Parallel] Epoch 75/300 Loss: 0.1290\n",
            "[Parallel] Epoch 76/300 Loss: 0.1286\n",
            "[Parallel] Epoch 77/300 Loss: 0.1288\n",
            "[Parallel] Epoch 78/300 Loss: 0.1284\n",
            "[Parallel] Epoch 79/300 Loss: 0.1283\n",
            "[Parallel] Epoch 80/300 Loss: 0.1285\n",
            "[Parallel] Epoch 81/300 Loss: 0.1277\n",
            "[Parallel] Epoch 82/300 Loss: 0.1279\n",
            "[Parallel] Epoch 83/300 Loss: 0.1277\n",
            "[Parallel] Epoch 84/300 Loss: 0.1278\n",
            "[Parallel] Epoch 85/300 Loss: 0.1272\n",
            "[Parallel] Epoch 86/300 Loss: 0.1282\n",
            "[Parallel] Epoch 87/300 Loss: 0.1271\n",
            "[Parallel] Epoch 88/300 Loss: 0.1271\n",
            "[Parallel] Epoch 89/300 Loss: 0.1272\n",
            "[Parallel] Epoch 90/300 Loss: 0.1271\n",
            "[Parallel] Epoch 91/300 Loss: 0.1272\n",
            "[Parallel] Epoch 92/300 Loss: 0.1269\n",
            "[Parallel] Epoch 93/300 Loss: 0.1262\n",
            "[Parallel] Epoch 94/300 Loss: 0.1267\n",
            "[Parallel] Epoch 95/300 Loss: 0.1263\n",
            "[Parallel] Epoch 96/300 Loss: 0.1259\n",
            "[Parallel] Epoch 97/300 Loss: 0.1262\n",
            "[Parallel] Epoch 98/300 Loss: 0.1263\n",
            "[Parallel] Epoch 99/300 Loss: 0.1259\n",
            "[Parallel] Epoch 100/300 Loss: 0.1256\n",
            "[Parallel] Epoch 101/300 Loss: 0.1260\n",
            "[Parallel] Epoch 102/300 Loss: 0.1255\n",
            "[Parallel] Epoch 103/300 Loss: 0.1252\n",
            "[Parallel] Epoch 104/300 Loss: 0.1253\n",
            "[Parallel] Epoch 105/300 Loss: 0.1252\n",
            "[Parallel] Epoch 106/300 Loss: 0.1250\n",
            "[Parallel] Epoch 107/300 Loss: 0.1252\n",
            "[Parallel] Epoch 108/300 Loss: 0.1246\n",
            "[Parallel] Epoch 109/300 Loss: 0.1247\n",
            "[Parallel] Epoch 110/300 Loss: 0.1249\n",
            "[Parallel] Epoch 111/300 Loss: 0.1249\n",
            "[Parallel] Epoch 112/300 Loss: 0.1247\n",
            "[Parallel] Epoch 113/300 Loss: 0.1239\n",
            "[Parallel] Epoch 114/300 Loss: 0.1247\n",
            "[Parallel] Epoch 115/300 Loss: 0.1239\n",
            "[Parallel] Epoch 116/300 Loss: 0.1235\n",
            "[Parallel] Epoch 117/300 Loss: 0.1238\n",
            "[Parallel] Epoch 118/300 Loss: 0.1236\n",
            "[Parallel] Epoch 119/300 Loss: 0.1240\n",
            "[Parallel] Epoch 120/300 Loss: 0.1233\n",
            "[Parallel] Epoch 121/300 Loss: 0.1235\n",
            "[Parallel] Epoch 122/300 Loss: 0.1237\n",
            "[Parallel] Epoch 123/300 Loss: 0.1223\n",
            "[Parallel] Epoch 124/300 Loss: 0.1226\n",
            "[Parallel] Epoch 125/300 Loss: 0.1225\n",
            "[Parallel] Epoch 126/300 Loss: 0.1225\n",
            "[Parallel] Epoch 127/300 Loss: 0.1225\n",
            "[Parallel] Epoch 128/300 Loss: 0.1225\n",
            "[Parallel] Epoch 129/300 Loss: 0.1225\n",
            "[Parallel] Epoch 130/300 Loss: 0.1218\n",
            "[Parallel] Epoch 131/300 Loss: 0.1220\n",
            "[Parallel] Epoch 132/300 Loss: 0.1224\n",
            "[Parallel] Epoch 133/300 Loss: 0.1219\n",
            "[Parallel] Epoch 134/300 Loss: 0.1215\n",
            "[Parallel] Epoch 135/300 Loss: 0.1214\n",
            "[Parallel] Epoch 136/300 Loss: 0.1215\n",
            "[Parallel] Epoch 137/300 Loss: 0.1212\n",
            "[Parallel] Epoch 138/300 Loss: 0.1218\n",
            "[Parallel] Epoch 139/300 Loss: 0.1209\n",
            "[Parallel] Epoch 140/300 Loss: 0.1211\n",
            "[Parallel] Epoch 141/300 Loss: 0.1212\n",
            "[Parallel] Epoch 142/300 Loss: 0.1206\n",
            "[Parallel] Epoch 143/300 Loss: 0.1207\n",
            "[Parallel] Epoch 144/300 Loss: 0.1208\n",
            "[Parallel] Epoch 145/300 Loss: 0.1206\n",
            "[Parallel] Epoch 146/300 Loss: 0.1200\n",
            "[Parallel] Epoch 147/300 Loss: 0.1203\n",
            "[Parallel] Epoch 148/300 Loss: 0.1200\n",
            "[Parallel] Epoch 149/300 Loss: 0.1199\n",
            "[Parallel] Epoch 150/300 Loss: 0.1195\n",
            "[Parallel] Epoch 151/300 Loss: 0.1198\n",
            "[Parallel] Epoch 152/300 Loss: 0.1196\n",
            "[Parallel] Epoch 153/300 Loss: 0.1192\n",
            "[Parallel] Epoch 154/300 Loss: 0.1194\n",
            "[Parallel] Epoch 155/300 Loss: 0.1192\n",
            "[Parallel] Epoch 156/300 Loss: 0.1191\n",
            "[Parallel] Epoch 157/300 Loss: 0.1194\n",
            "[Parallel] Epoch 158/300 Loss: 0.1188\n",
            "[Parallel] Epoch 159/300 Loss: 0.1187\n",
            "[Parallel] Epoch 160/300 Loss: 0.1194\n",
            "[Parallel] Epoch 161/300 Loss: 0.1188\n",
            "[Parallel] Epoch 162/300 Loss: 0.1184\n",
            "[Parallel] Epoch 163/300 Loss: 0.1183\n",
            "[Parallel] Epoch 164/300 Loss: 0.1182\n",
            "[Parallel] Epoch 165/300 Loss: 0.1182\n",
            "[Parallel] Epoch 166/300 Loss: 0.1183\n",
            "[Parallel] Epoch 167/300 Loss: 0.1180\n",
            "[Parallel] Epoch 168/300 Loss: 0.1180\n",
            "[Parallel] Epoch 169/300 Loss: 0.1180\n",
            "[Parallel] Epoch 170/300 Loss: 0.1179\n",
            "[Parallel] Epoch 171/300 Loss: 0.1175\n",
            "[Parallel] Epoch 172/300 Loss: 0.1173\n",
            "[Parallel] Epoch 173/300 Loss: 0.1172\n",
            "[Parallel] Epoch 174/300 Loss: 0.1174\n",
            "[Parallel] Epoch 175/300 Loss: 0.1171\n",
            "[Parallel] Epoch 176/300 Loss: 0.1173\n",
            "[Parallel] Epoch 177/300 Loss: 0.1170\n",
            "[Parallel] Epoch 178/300 Loss: 0.1174\n",
            "[Parallel] Epoch 179/300 Loss: 0.1164\n",
            "[Parallel] Epoch 180/300 Loss: 0.1163\n",
            "[Parallel] Epoch 181/300 Loss: 0.1163\n",
            "[Parallel] Epoch 182/300 Loss: 0.1163\n",
            "[Parallel] Epoch 183/300 Loss: 0.1162\n",
            "[Parallel] Epoch 184/300 Loss: 0.1162\n",
            "[Parallel] Epoch 185/300 Loss: 0.1160\n",
            "[Parallel] Epoch 186/300 Loss: 0.1159\n",
            "[Parallel] Epoch 187/300 Loss: 0.1158\n",
            "[Parallel] Epoch 188/300 Loss: 0.1156\n",
            "[Parallel] Epoch 189/300 Loss: 0.1159\n",
            "[Parallel] Epoch 190/300 Loss: 0.1153\n",
            "[Parallel] Epoch 191/300 Loss: 0.1156\n",
            "[Parallel] Epoch 192/300 Loss: 0.1155\n",
            "[Parallel] Epoch 193/300 Loss: 0.1154\n",
            "[Parallel] Epoch 194/300 Loss: 0.1152\n",
            "[Parallel] Epoch 195/300 Loss: 0.1152\n",
            "[Parallel] Epoch 196/300 Loss: 0.1148\n",
            "[Parallel] Epoch 197/300 Loss: 0.1144\n",
            "[Parallel] Epoch 198/300 Loss: 0.1147\n",
            "[Parallel] Epoch 199/300 Loss: 0.1146\n",
            "[Parallel] Epoch 200/300 Loss: 0.1141\n",
            "[Parallel] Epoch 201/300 Loss: 0.1139\n",
            "[Parallel] Epoch 202/300 Loss: 0.1143\n",
            "[Parallel] Epoch 203/300 Loss: 0.1143\n",
            "[Parallel] Epoch 204/300 Loss: 0.1140\n",
            "[Parallel] Epoch 205/300 Loss: 0.1135\n",
            "[Parallel] Epoch 206/300 Loss: 0.1140\n",
            "[Parallel] Epoch 207/300 Loss: 0.1138\n",
            "[Parallel] Epoch 208/300 Loss: 0.1141\n",
            "[Parallel] Epoch 209/300 Loss: 0.1137\n",
            "[Parallel] Epoch 210/300 Loss: 0.1132\n",
            "[Parallel] Epoch 211/300 Loss: 0.1130\n",
            "[Parallel] Epoch 212/300 Loss: 0.1130\n",
            "[Parallel] Epoch 213/300 Loss: 0.1132\n",
            "[Parallel] Epoch 214/300 Loss: 0.1127\n",
            "[Parallel] Epoch 215/300 Loss: 0.1129\n",
            "[Parallel] Epoch 216/300 Loss: 0.1124\n",
            "[Parallel] Epoch 217/300 Loss: 0.1126\n",
            "[Parallel] Epoch 218/300 Loss: 0.1122\n",
            "[Parallel] Epoch 219/300 Loss: 0.1124\n",
            "[Parallel] Epoch 220/300 Loss: 0.1120\n",
            "[Parallel] Epoch 221/300 Loss: 0.1123\n",
            "[Parallel] Epoch 222/300 Loss: 0.1117\n",
            "[Parallel] Epoch 223/300 Loss: 0.1118\n",
            "[Parallel] Epoch 224/300 Loss: 0.1115\n",
            "[Parallel] Epoch 225/300 Loss: 0.1116\n",
            "[Parallel] Epoch 226/300 Loss: 0.1120\n",
            "[Parallel] Epoch 227/300 Loss: 0.1115\n",
            "[Parallel] Epoch 228/300 Loss: 0.1119\n",
            "[Parallel] Epoch 229/300 Loss: 0.1113\n",
            "[Parallel] Epoch 230/300 Loss: 0.1112\n",
            "[Parallel] Epoch 231/300 Loss: 0.1111\n",
            "[Parallel] Epoch 232/300 Loss: 0.1115\n",
            "[Parallel] Epoch 233/300 Loss: 0.1110\n",
            "[Parallel] Epoch 234/300 Loss: 0.1118\n",
            "[Parallel] Epoch 235/300 Loss: 0.1106\n",
            "[Parallel] Epoch 236/300 Loss: 0.1104\n",
            "[Parallel] Epoch 237/300 Loss: 0.1103\n",
            "[Parallel] Epoch 238/300 Loss: 0.1103\n",
            "[Parallel] Epoch 239/300 Loss: 0.1100\n",
            "[Parallel] Epoch 240/300 Loss: 0.1102\n",
            "[Parallel] Epoch 241/300 Loss: 0.1099\n",
            "[Parallel] Epoch 242/300 Loss: 0.1103\n",
            "[Parallel] Epoch 243/300 Loss: 0.1100\n",
            "[Parallel] Epoch 244/300 Loss: 0.1101\n",
            "[Parallel] Epoch 245/300 Loss: 0.1097\n",
            "[Parallel] Epoch 246/300 Loss: 0.1099\n",
            "[Parallel] Epoch 247/300 Loss: 0.1095\n",
            "[Parallel] Epoch 248/300 Loss: 0.1091\n",
            "[Parallel] Epoch 249/300 Loss: 0.1091\n",
            "[Parallel] Epoch 250/300 Loss: 0.1092\n",
            "[Parallel] Epoch 251/300 Loss: 0.1093\n",
            "[Parallel] Epoch 252/300 Loss: 0.1089\n",
            "[Parallel] Epoch 253/300 Loss: 0.1085\n",
            "[Parallel] Epoch 254/300 Loss: 0.1088\n",
            "[Parallel] Epoch 255/300 Loss: 0.1088\n",
            "[Parallel] Epoch 256/300 Loss: 0.1088\n",
            "[Parallel] Epoch 257/300 Loss: 0.1090\n",
            "[Parallel] Epoch 258/300 Loss: 0.1084\n",
            "[Parallel] Epoch 259/300 Loss: 0.1081\n",
            "[Parallel] Epoch 260/300 Loss: 0.1080\n",
            "[Parallel] Epoch 261/300 Loss: 0.1079\n",
            "[Parallel] Epoch 262/300 Loss: 0.1079\n",
            "[Parallel] Epoch 263/300 Loss: 0.1078\n",
            "[Parallel] Epoch 264/300 Loss: 0.1083\n",
            "[Parallel] Epoch 265/300 Loss: 0.1076\n",
            "[Parallel] Epoch 266/300 Loss: 0.1074\n",
            "[Parallel] Epoch 267/300 Loss: 0.1076\n",
            "[Parallel] Epoch 268/300 Loss: 0.1077\n",
            "[Parallel] Epoch 269/300 Loss: 0.1071\n",
            "[Parallel] Epoch 270/300 Loss: 0.1069\n",
            "[Parallel] Epoch 271/300 Loss: 0.1074\n",
            "[Parallel] Epoch 272/300 Loss: 0.1067\n",
            "[Parallel] Epoch 273/300 Loss: 0.1075\n",
            "[Parallel] Epoch 274/300 Loss: 0.1066\n",
            "[Parallel] Epoch 275/300 Loss: 0.1070\n",
            "[Parallel] Epoch 276/300 Loss: 0.1066\n",
            "[Parallel] Epoch 277/300 Loss: 0.1064\n",
            "[Parallel] Epoch 278/300 Loss: 0.1059\n",
            "[Parallel] Epoch 279/300 Loss: 0.1063\n",
            "[Parallel] Epoch 280/300 Loss: 0.1061\n",
            "[Parallel] Epoch 281/300 Loss: 0.1058\n",
            "[Parallel] Epoch 282/300 Loss: 0.1063\n",
            "[Parallel] Epoch 283/300 Loss: 0.1058\n",
            "[Parallel] Epoch 284/300 Loss: 0.1062\n",
            "[Parallel] Epoch 285/300 Loss: 0.1053\n",
            "[Parallel] Epoch 286/300 Loss: 0.1057\n",
            "[Parallel] Epoch 287/300 Loss: 0.1056\n",
            "[Parallel] Epoch 288/300 Loss: 0.1055\n",
            "[Parallel] Epoch 289/300 Loss: 0.1051\n",
            "[Parallel] Epoch 290/300 Loss: 0.1049\n",
            "[Parallel] Epoch 291/300 Loss: 0.1051\n",
            "[Parallel] Epoch 292/300 Loss: 0.1054\n",
            "[Parallel] Epoch 293/300 Loss: 0.1046\n",
            "[Parallel] Epoch 294/300 Loss: 0.1055\n",
            "[Parallel] Epoch 295/300 Loss: 0.1045\n",
            "[Parallel] Epoch 296/300 Loss: 0.1049\n",
            "[Parallel] Epoch 297/300 Loss: 0.1041\n",
            "[Parallel] Epoch 298/300 Loss: 0.1040\n",
            "[Parallel] Epoch 299/300 Loss: 0.1038\n",
            "[Parallel] Epoch 300/300 Loss: 0.1035\n",
            "⏱️ Total training time for Parallel Model: 165.85 seconds\n",
            "\n",
            "🔁 Training Non-Parallel Model\n",
            "[Sequential] Epoch 1/300 Loss: 1.5485\n",
            "[Sequential] Epoch 2/300 Loss: 0.1543\n",
            "[Sequential] Epoch 3/300 Loss: 0.1470\n",
            "[Sequential] Epoch 4/300 Loss: 0.1439\n",
            "[Sequential] Epoch 5/300 Loss: 0.1425\n",
            "[Sequential] Epoch 6/300 Loss: 0.1413\n",
            "[Sequential] Epoch 7/300 Loss: 0.1403\n",
            "[Sequential] Epoch 8/300 Loss: 0.1397\n",
            "[Sequential] Epoch 9/300 Loss: 0.1389\n",
            "[Sequential] Epoch 10/300 Loss: 0.1383\n",
            "[Sequential] Epoch 11/300 Loss: 0.1377\n",
            "[Sequential] Epoch 12/300 Loss: 0.1371\n",
            "[Sequential] Epoch 13/300 Loss: 0.1374\n",
            "[Sequential] Epoch 14/300 Loss: 0.1370\n",
            "[Sequential] Epoch 15/300 Loss: 0.1361\n",
            "[Sequential] Epoch 16/300 Loss: 0.1365\n",
            "[Sequential] Epoch 17/300 Loss: 0.1359\n",
            "[Sequential] Epoch 18/300 Loss: 0.1357\n",
            "[Sequential] Epoch 19/300 Loss: 0.1354\n",
            "[Sequential] Epoch 20/300 Loss: 0.1348\n",
            "[Sequential] Epoch 21/300 Loss: 0.1350\n",
            "[Sequential] Epoch 22/300 Loss: 0.1350\n",
            "[Sequential] Epoch 23/300 Loss: 0.1344\n",
            "[Sequential] Epoch 24/300 Loss: 0.1340\n",
            "[Sequential] Epoch 25/300 Loss: 0.1339\n",
            "[Sequential] Epoch 26/300 Loss: 0.1338\n",
            "[Sequential] Epoch 27/300 Loss: 0.1339\n",
            "[Sequential] Epoch 28/300 Loss: 0.1337\n",
            "[Sequential] Epoch 29/300 Loss: 0.1336\n",
            "[Sequential] Epoch 30/300 Loss: 0.1330\n",
            "[Sequential] Epoch 31/300 Loss: 0.1333\n",
            "[Sequential] Epoch 32/300 Loss: 0.1332\n",
            "[Sequential] Epoch 33/300 Loss: 0.1329\n",
            "[Sequential] Epoch 34/300 Loss: 0.1328\n",
            "[Sequential] Epoch 35/300 Loss: 0.1327\n",
            "[Sequential] Epoch 36/300 Loss: 0.1330\n",
            "[Sequential] Epoch 37/300 Loss: 0.1324\n",
            "[Sequential] Epoch 38/300 Loss: 0.1323\n",
            "[Sequential] Epoch 39/300 Loss: 0.1326\n",
            "[Sequential] Epoch 40/300 Loss: 0.1320\n",
            "[Sequential] Epoch 41/300 Loss: 0.1320\n",
            "[Sequential] Epoch 42/300 Loss: 0.1318\n",
            "[Sequential] Epoch 43/300 Loss: 0.1318\n",
            "[Sequential] Epoch 44/300 Loss: 0.1319\n",
            "[Sequential] Epoch 45/300 Loss: 0.1311\n",
            "[Sequential] Epoch 46/300 Loss: 0.1309\n",
            "[Sequential] Epoch 47/300 Loss: 0.1312\n",
            "[Sequential] Epoch 48/300 Loss: 0.1310\n",
            "[Sequential] Epoch 49/300 Loss: 0.1313\n",
            "[Sequential] Epoch 50/300 Loss: 0.1309\n",
            "[Sequential] Epoch 51/300 Loss: 0.1311\n",
            "[Sequential] Epoch 52/300 Loss: 0.1308\n",
            "[Sequential] Epoch 53/300 Loss: 0.1305\n",
            "[Sequential] Epoch 54/300 Loss: 0.1305\n",
            "[Sequential] Epoch 55/300 Loss: 0.1299\n",
            "[Sequential] Epoch 56/300 Loss: 0.1304\n",
            "[Sequential] Epoch 57/300 Loss: 0.1299\n",
            "[Sequential] Epoch 58/300 Loss: 0.1297\n",
            "[Sequential] Epoch 59/300 Loss: 0.1296\n",
            "[Sequential] Epoch 60/300 Loss: 0.1296\n",
            "[Sequential] Epoch 61/300 Loss: 0.1293\n",
            "[Sequential] Epoch 62/300 Loss: 0.1294\n",
            "[Sequential] Epoch 63/300 Loss: 0.1297\n",
            "[Sequential] Epoch 64/300 Loss: 0.1291\n",
            "[Sequential] Epoch 65/300 Loss: 0.1296\n",
            "[Sequential] Epoch 66/300 Loss: 0.1290\n",
            "[Sequential] Epoch 67/300 Loss: 0.1289\n",
            "[Sequential] Epoch 68/300 Loss: 0.1297\n",
            "[Sequential] Epoch 69/300 Loss: 0.1294\n",
            "[Sequential] Epoch 70/300 Loss: 0.1285\n",
            "[Sequential] Epoch 71/300 Loss: 0.1279\n",
            "[Sequential] Epoch 72/300 Loss: 0.1286\n",
            "[Sequential] Epoch 73/300 Loss: 0.1280\n",
            "[Sequential] Epoch 74/300 Loss: 0.1280\n",
            "[Sequential] Epoch 75/300 Loss: 0.1276\n",
            "[Sequential] Epoch 76/300 Loss: 0.1277\n",
            "[Sequential] Epoch 77/300 Loss: 0.1274\n",
            "[Sequential] Epoch 78/300 Loss: 0.1273\n",
            "[Sequential] Epoch 79/300 Loss: 0.1276\n",
            "[Sequential] Epoch 80/300 Loss: 0.1271\n",
            "[Sequential] Epoch 81/300 Loss: 0.1266\n",
            "[Sequential] Epoch 82/300 Loss: 0.1265\n",
            "[Sequential] Epoch 83/300 Loss: 0.1264\n",
            "[Sequential] Epoch 84/300 Loss: 0.1265\n",
            "[Sequential] Epoch 85/300 Loss: 0.1264\n",
            "[Sequential] Epoch 86/300 Loss: 0.1265\n",
            "[Sequential] Epoch 87/300 Loss: 0.1264\n",
            "[Sequential] Epoch 88/300 Loss: 0.1264\n",
            "[Sequential] Epoch 89/300 Loss: 0.1254\n",
            "[Sequential] Epoch 90/300 Loss: 0.1259\n",
            "[Sequential] Epoch 91/300 Loss: 0.1260\n",
            "[Sequential] Epoch 92/300 Loss: 0.1253\n",
            "[Sequential] Epoch 93/300 Loss: 0.1255\n",
            "[Sequential] Epoch 94/300 Loss: 0.1254\n",
            "[Sequential] Epoch 95/300 Loss: 0.1258\n",
            "[Sequential] Epoch 96/300 Loss: 0.1249\n",
            "[Sequential] Epoch 97/300 Loss: 0.1249\n",
            "[Sequential] Epoch 98/300 Loss: 0.1248\n",
            "[Sequential] Epoch 99/300 Loss: 0.1243\n",
            "[Sequential] Epoch 100/300 Loss: 0.1246\n",
            "[Sequential] Epoch 101/300 Loss: 0.1246\n",
            "[Sequential] Epoch 102/300 Loss: 0.1244\n",
            "[Sequential] Epoch 103/300 Loss: 0.1243\n",
            "[Sequential] Epoch 104/300 Loss: 0.1245\n",
            "[Sequential] Epoch 105/300 Loss: 0.1239\n",
            "[Sequential] Epoch 106/300 Loss: 0.1243\n",
            "[Sequential] Epoch 107/300 Loss: 0.1235\n",
            "[Sequential] Epoch 108/300 Loss: 0.1234\n",
            "[Sequential] Epoch 109/300 Loss: 0.1232\n",
            "[Sequential] Epoch 110/300 Loss: 0.1230\n",
            "[Sequential] Epoch 111/300 Loss: 0.1232\n",
            "[Sequential] Epoch 112/300 Loss: 0.1235\n",
            "[Sequential] Epoch 113/300 Loss: 0.1228\n",
            "[Sequential] Epoch 114/300 Loss: 0.1229\n",
            "[Sequential] Epoch 115/300 Loss: 0.1229\n",
            "[Sequential] Epoch 116/300 Loss: 0.1229\n",
            "[Sequential] Epoch 117/300 Loss: 0.1222\n",
            "[Sequential] Epoch 118/300 Loss: 0.1224\n",
            "[Sequential] Epoch 119/300 Loss: 0.1222\n",
            "[Sequential] Epoch 120/300 Loss: 0.1220\n",
            "[Sequential] Epoch 121/300 Loss: 0.1220\n",
            "[Sequential] Epoch 122/300 Loss: 0.1217\n",
            "[Sequential] Epoch 123/300 Loss: 0.1215\n",
            "[Sequential] Epoch 124/300 Loss: 0.1219\n",
            "[Sequential] Epoch 125/300 Loss: 0.1218\n",
            "[Sequential] Epoch 126/300 Loss: 0.1217\n",
            "[Sequential] Epoch 127/300 Loss: 0.1210\n",
            "[Sequential] Epoch 128/300 Loss: 0.1208\n",
            "[Sequential] Epoch 129/300 Loss: 0.1214\n",
            "[Sequential] Epoch 130/300 Loss: 0.1207\n",
            "[Sequential] Epoch 131/300 Loss: 0.1202\n",
            "[Sequential] Epoch 132/300 Loss: 0.1205\n",
            "[Sequential] Epoch 133/300 Loss: 0.1207\n",
            "[Sequential] Epoch 134/300 Loss: 0.1202\n",
            "[Sequential] Epoch 135/300 Loss: 0.1200\n",
            "[Sequential] Epoch 136/300 Loss: 0.1202\n",
            "[Sequential] Epoch 137/300 Loss: 0.1202\n",
            "[Sequential] Epoch 138/300 Loss: 0.1198\n",
            "[Sequential] Epoch 139/300 Loss: 0.1195\n",
            "[Sequential] Epoch 140/300 Loss: 0.1197\n",
            "[Sequential] Epoch 141/300 Loss: 0.1193\n",
            "[Sequential] Epoch 142/300 Loss: 0.1192\n",
            "[Sequential] Epoch 143/300 Loss: 0.1189\n",
            "[Sequential] Epoch 144/300 Loss: 0.1193\n",
            "[Sequential] Epoch 145/300 Loss: 0.1192\n",
            "[Sequential] Epoch 146/300 Loss: 0.1185\n",
            "[Sequential] Epoch 147/300 Loss: 0.1182\n",
            "[Sequential] Epoch 148/300 Loss: 0.1184\n",
            "[Sequential] Epoch 149/300 Loss: 0.1185\n",
            "[Sequential] Epoch 150/300 Loss: 0.1180\n",
            "[Sequential] Epoch 151/300 Loss: 0.1189\n",
            "[Sequential] Epoch 152/300 Loss: 0.1181\n",
            "[Sequential] Epoch 153/300 Loss: 0.1183\n",
            "[Sequential] Epoch 154/300 Loss: 0.1180\n",
            "[Sequential] Epoch 155/300 Loss: 0.1176\n",
            "[Sequential] Epoch 156/300 Loss: 0.1174\n",
            "[Sequential] Epoch 157/300 Loss: 0.1176\n",
            "[Sequential] Epoch 158/300 Loss: 0.1174\n",
            "[Sequential] Epoch 159/300 Loss: 0.1176\n",
            "[Sequential] Epoch 160/300 Loss: 0.1170\n",
            "[Sequential] Epoch 161/300 Loss: 0.1169\n",
            "[Sequential] Epoch 162/300 Loss: 0.1170\n",
            "[Sequential] Epoch 163/300 Loss: 0.1173\n",
            "[Sequential] Epoch 164/300 Loss: 0.1168\n",
            "[Sequential] Epoch 165/300 Loss: 0.1175\n",
            "[Sequential] Epoch 166/300 Loss: 0.1168\n",
            "[Sequential] Epoch 167/300 Loss: 0.1164\n",
            "[Sequential] Epoch 168/300 Loss: 0.1167\n",
            "[Sequential] Epoch 169/300 Loss: 0.1164\n",
            "[Sequential] Epoch 170/300 Loss: 0.1159\n",
            "[Sequential] Epoch 171/300 Loss: 0.1162\n",
            "[Sequential] Epoch 172/300 Loss: 0.1159\n",
            "[Sequential] Epoch 173/300 Loss: 0.1158\n",
            "[Sequential] Epoch 174/300 Loss: 0.1157\n",
            "[Sequential] Epoch 175/300 Loss: 0.1157\n",
            "[Sequential] Epoch 176/300 Loss: 0.1155\n",
            "[Sequential] Epoch 177/300 Loss: 0.1155\n",
            "[Sequential] Epoch 178/300 Loss: 0.1151\n",
            "[Sequential] Epoch 179/300 Loss: 0.1148\n",
            "[Sequential] Epoch 180/300 Loss: 0.1150\n",
            "[Sequential] Epoch 181/300 Loss: 0.1146\n",
            "[Sequential] Epoch 182/300 Loss: 0.1151\n",
            "[Sequential] Epoch 183/300 Loss: 0.1151\n",
            "[Sequential] Epoch 184/300 Loss: 0.1146\n",
            "[Sequential] Epoch 185/300 Loss: 0.1145\n",
            "[Sequential] Epoch 186/300 Loss: 0.1145\n",
            "[Sequential] Epoch 187/300 Loss: 0.1137\n",
            "[Sequential] Epoch 188/300 Loss: 0.1140\n",
            "[Sequential] Epoch 189/300 Loss: 0.1139\n",
            "[Sequential] Epoch 190/300 Loss: 0.1136\n",
            "[Sequential] Epoch 191/300 Loss: 0.1137\n",
            "[Sequential] Epoch 192/300 Loss: 0.1140\n",
            "[Sequential] Epoch 193/300 Loss: 0.1134\n",
            "[Sequential] Epoch 194/300 Loss: 0.1134\n",
            "[Sequential] Epoch 195/300 Loss: 0.1134\n",
            "[Sequential] Epoch 196/300 Loss: 0.1131\n",
            "[Sequential] Epoch 197/300 Loss: 0.1138\n",
            "[Sequential] Epoch 198/300 Loss: 0.1127\n",
            "[Sequential] Epoch 199/300 Loss: 0.1127\n",
            "[Sequential] Epoch 200/300 Loss: 0.1126\n",
            "[Sequential] Epoch 201/300 Loss: 0.1126\n",
            "[Sequential] Epoch 202/300 Loss: 0.1129\n",
            "[Sequential] Epoch 203/300 Loss: 0.1121\n",
            "[Sequential] Epoch 204/300 Loss: 0.1119\n",
            "[Sequential] Epoch 205/300 Loss: 0.1130\n",
            "[Sequential] Epoch 206/300 Loss: 0.1116\n",
            "[Sequential] Epoch 207/300 Loss: 0.1122\n",
            "[Sequential] Epoch 208/300 Loss: 0.1116\n",
            "[Sequential] Epoch 209/300 Loss: 0.1115\n",
            "[Sequential] Epoch 210/300 Loss: 0.1115\n",
            "[Sequential] Epoch 211/300 Loss: 0.1114\n",
            "[Sequential] Epoch 212/300 Loss: 0.1113\n",
            "[Sequential] Epoch 213/300 Loss: 0.1115\n",
            "[Sequential] Epoch 214/300 Loss: 0.1116\n",
            "[Sequential] Epoch 215/300 Loss: 0.1112\n",
            "[Sequential] Epoch 216/300 Loss: 0.1111\n",
            "[Sequential] Epoch 217/300 Loss: 0.1112\n",
            "[Sequential] Epoch 218/300 Loss: 0.1107\n",
            "[Sequential] Epoch 219/300 Loss: 0.1111\n",
            "[Sequential] Epoch 220/300 Loss: 0.1106\n",
            "[Sequential] Epoch 221/300 Loss: 0.1105\n",
            "[Sequential] Epoch 222/300 Loss: 0.1107\n",
            "[Sequential] Epoch 223/300 Loss: 0.1101\n",
            "[Sequential] Epoch 224/300 Loss: 0.1103\n",
            "[Sequential] Epoch 225/300 Loss: 0.1103\n",
            "[Sequential] Epoch 226/300 Loss: 0.1101\n",
            "[Sequential] Epoch 227/300 Loss: 0.1097\n",
            "[Sequential] Epoch 228/300 Loss: 0.1095\n",
            "[Sequential] Epoch 229/300 Loss: 0.1101\n",
            "[Sequential] Epoch 230/300 Loss: 0.1094\n",
            "[Sequential] Epoch 231/300 Loss: 0.1097\n",
            "[Sequential] Epoch 232/300 Loss: 0.1094\n",
            "[Sequential] Epoch 233/300 Loss: 0.1090\n",
            "[Sequential] Epoch 234/300 Loss: 0.1090\n",
            "[Sequential] Epoch 235/300 Loss: 0.1088\n",
            "[Sequential] Epoch 236/300 Loss: 0.1089\n",
            "[Sequential] Epoch 237/300 Loss: 0.1089\n",
            "[Sequential] Epoch 238/300 Loss: 0.1092\n",
            "[Sequential] Epoch 239/300 Loss: 0.1089\n",
            "[Sequential] Epoch 240/300 Loss: 0.1088\n",
            "[Sequential] Epoch 241/300 Loss: 0.1085\n",
            "[Sequential] Epoch 242/300 Loss: 0.1084\n",
            "[Sequential] Epoch 243/300 Loss: 0.1078\n",
            "[Sequential] Epoch 244/300 Loss: 0.1084\n",
            "[Sequential] Epoch 245/300 Loss: 0.1080\n",
            "[Sequential] Epoch 246/300 Loss: 0.1075\n",
            "[Sequential] Epoch 247/300 Loss: 0.1077\n",
            "[Sequential] Epoch 248/300 Loss: 0.1081\n",
            "[Sequential] Epoch 249/300 Loss: 0.1080\n",
            "[Sequential] Epoch 250/300 Loss: 0.1075\n",
            "[Sequential] Epoch 251/300 Loss: 0.1079\n",
            "[Sequential] Epoch 252/300 Loss: 0.1075\n",
            "[Sequential] Epoch 253/300 Loss: 0.1072\n",
            "[Sequential] Epoch 254/300 Loss: 0.1068\n",
            "[Sequential] Epoch 255/300 Loss: 0.1073\n",
            "[Sequential] Epoch 256/300 Loss: 0.1069\n",
            "[Sequential] Epoch 257/300 Loss: 0.1066\n",
            "[Sequential] Epoch 258/300 Loss: 0.1068\n",
            "[Sequential] Epoch 259/300 Loss: 0.1067\n",
            "[Sequential] Epoch 260/300 Loss: 0.1068\n",
            "[Sequential] Epoch 261/300 Loss: 0.1069\n",
            "[Sequential] Epoch 262/300 Loss: 0.1058\n",
            "[Sequential] Epoch 263/300 Loss: 0.1063\n",
            "[Sequential] Epoch 264/300 Loss: 0.1060\n",
            "[Sequential] Epoch 265/300 Loss: 0.1063\n",
            "[Sequential] Epoch 266/300 Loss: 0.1055\n",
            "[Sequential] Epoch 267/300 Loss: 0.1060\n",
            "[Sequential] Epoch 268/300 Loss: 0.1055\n",
            "[Sequential] Epoch 269/300 Loss: 0.1054\n",
            "[Sequential] Epoch 270/300 Loss: 0.1054\n",
            "[Sequential] Epoch 271/300 Loss: 0.1058\n",
            "[Sequential] Epoch 272/300 Loss: 0.1053\n",
            "[Sequential] Epoch 273/300 Loss: 0.1053\n",
            "[Sequential] Epoch 274/300 Loss: 0.1052\n",
            "[Sequential] Epoch 275/300 Loss: 0.1048\n",
            "[Sequential] Epoch 276/300 Loss: 0.1051\n",
            "[Sequential] Epoch 277/300 Loss: 0.1053\n",
            "[Sequential] Epoch 278/300 Loss: 0.1050\n",
            "[Sequential] Epoch 279/300 Loss: 0.1049\n",
            "[Sequential] Epoch 280/300 Loss: 0.1046\n",
            "[Sequential] Epoch 281/300 Loss: 0.1041\n",
            "[Sequential] Epoch 282/300 Loss: 0.1044\n",
            "[Sequential] Epoch 283/300 Loss: 0.1047\n",
            "[Sequential] Epoch 284/300 Loss: 0.1041\n",
            "[Sequential] Epoch 285/300 Loss: 0.1038\n",
            "[Sequential] Epoch 286/300 Loss: 0.1042\n",
            "[Sequential] Epoch 287/300 Loss: 0.1041\n",
            "[Sequential] Epoch 288/300 Loss: 0.1040\n",
            "[Sequential] Epoch 289/300 Loss: 0.1035\n",
            "[Sequential] Epoch 290/300 Loss: 0.1036\n",
            "[Sequential] Epoch 291/300 Loss: 0.1043\n",
            "[Sequential] Epoch 292/300 Loss: 0.1029\n",
            "[Sequential] Epoch 293/300 Loss: 0.1036\n",
            "[Sequential] Epoch 294/300 Loss: 0.1035\n",
            "[Sequential] Epoch 295/300 Loss: 0.1035\n",
            "[Sequential] Epoch 296/300 Loss: 0.1031\n",
            "[Sequential] Epoch 297/300 Loss: 0.1029\n",
            "[Sequential] Epoch 298/300 Loss: 0.1029\n",
            "[Sequential] Epoch 299/300 Loss: 0.1029\n",
            "[Sequential] Epoch 300/300 Loss: 0.1030\n",
            "⏱️ Total training time for Non-Parallel Model: 172.07 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step, prdict the next week of crimes for each AREA NAME, We'll\n",
        "* Grab the last N weeks for each AREA\n",
        "* Predict the next week's crime count vector\n",
        "* Optionally visualize the output"
      ],
      "metadata": {
        "id": "pZ8Yy-N96Fn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_next_week(model, df, input_window=4):\n",
        "    model.eval()\n",
        "    areas = df['AREA NAME'].unique()\n",
        "    feature_cols = [col for col in df.columns if col not in ['AREA NAME', 'WEEK_START']]\n",
        "\n",
        "    predictions = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for area in areas:\n",
        "            area_df = df[df['AREA NAME'] == area].sort_values('WEEK_START')\n",
        "            recent_data = area_df[feature_cols].values[-input_window:]\n",
        "\n",
        "            if len(recent_data) < input_window:\n",
        "                continue\n",
        "\n",
        "            x = torch.tensor(recent_data, dtype=torch.float32).unsqueeze(0).to(\"cuda\")\n",
        "            y_pred = model(x).cpu().numpy().flatten()\n",
        "\n",
        "            y_pred = np.expm1(y_pred)              # Convert back from log-space\n",
        "            y_pred = np.clip(y_pred, 0, None)      # Clamp to avoid negatives\n",
        "            y_pred = y_pred.round().astype(int)    # Round to integers\n",
        "\n",
        "            crime_pred = dict(zip(feature_cols, y_pred))\n",
        "            predictions[area] = crime_pred\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "vbUrk42T6Kk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crime_forecast = predict_next_week(model, weekly_pivot, input_window=20)\n",
        "\n",
        "#Example printout\n",
        "for area, pred in list(crime_forecast.items())[:]: #First 5 areas\n",
        "  print(f'\\n {area}')\n",
        "  for crime, count in pred.items():\n",
        "    print(f' - {crime}: {count}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3hG2el57uVK",
        "outputId": "9722da32-a819-4168-9ca9-9c8ef2e78575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 77th Street\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 0\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 2\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 5\n",
            "\n",
            " Central\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 13\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 3\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 7\n",
            " - Threats: 0\n",
            " - Vandalism: 5\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 21\n",
            "\n",
            " Devonshire\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 1\n",
            " - Threats: 0\n",
            " - Vandalism: 0\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 5\n",
            "\n",
            " Foothill\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 0\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 1\n",
            " - Threats: 0\n",
            " - Vandalism: 0\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 4\n",
            "\n",
            " Harbor\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 3\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 3\n",
            " - Threats: 0\n",
            " - Vandalism: 2\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 14\n",
            "\n",
            " Hollenbeck\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 4\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 9\n",
            "\n",
            " Hollywood\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 2\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 2\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 3\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 10\n",
            "\n",
            " Mission\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 0\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 2\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 4\n",
            "\n",
            " N Hollywood\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 0\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 0\n",
            " - Threats: 0\n",
            " - Vandalism: 0\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 2\n",
            "\n",
            " Newton\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 2\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 2\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 6\n",
            "\n",
            " Northeast\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 2\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 6\n",
            "\n",
            " Olympic\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 2\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 0\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 1\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 5\n",
            "\n",
            " Pacific\n",
            " - Arson: 0\n",
            " - Assault: 1\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 1\n",
            " - Sexual Offense: 0\n",
            " - Theft: 1\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 3\n",
            "\n",
            " Rampart\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 0\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 0\n",
            " - Threats: 0\n",
            " - Vandalism: 0\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 2\n",
            "\n",
            " Southeast\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 2\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 1\n",
            " - Weapons: 0\n",
            " - Total Crimes: 5\n",
            "\n",
            " Southwest\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 0\n",
            " - Threats: 0\n",
            " - Vandalism: 0\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 2\n",
            "\n",
            " Topanga\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 0\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 2\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 5\n",
            "\n",
            " Van Nuys\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 2\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 5\n",
            "\n",
            " West LA\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 2\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 6\n",
            "\n",
            " West Valley\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 1\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 0\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 1\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 5\n",
            "\n",
            " Wilshire\n",
            " - Arson: 0\n",
            " - Assault: 0\n",
            " - Burglary: 2\n",
            " - Fraud: 0\n",
            " - Homicide: 0\n",
            " - Other: 1\n",
            " - Robbery: 0\n",
            " - Sexual Offense: 0\n",
            " - Theft: 4\n",
            " - Threats: 0\n",
            " - Vandalism: 1\n",
            " - Vehicle Theft: 0\n",
            " - Weapons: 0\n",
            " - Total Crimes: 9\n"
          ]
        }
      ]
    }
  ]
}