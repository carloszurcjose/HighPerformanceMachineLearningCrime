{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7biZG_TOU0E-",
        "outputId": "e47657fb-ba75-4315-8e4b-906688ce1d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All experiments completed. Results saved to 'performance_results.csv'.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 🚀 Enable GPU & Optimizations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/HighPerformanceMachineLearning/CrimeDatafrom2020toPresent.csv')\n",
        "\n",
        "# Convert DATE OCC to datetime format\n",
        "df['DATE OCC'] = pd.to_datetime(df['DATE OCC'], errors='coerce')\n",
        "\n",
        "# Extract time features\n",
        "df['Year_OCC'] = df['DATE OCC'].dt.year\n",
        "df['Month_OCC'] = df['DATE OCC'].dt.month\n",
        "df['Day_OCC'] = df['DATE OCC'].dt.day\n",
        "df['Hour_OCC'] = df['TIME OCC'] // 100  # Convert HHMM to hours\n",
        "\n",
        "# Count crimes per location\n",
        "crime_counts = df.groupby(['LAT', 'LON']).size()\n",
        "\n",
        "# Assign Crime Risk Scores (1-10) and scale to 1-5\n",
        "df['Crime_Risk'] = df.set_index(['LAT', 'LON']).index.map(lambda x: crime_counts.get(x, 0))\n",
        "df['Crime_Risk'] = np.ceil(10 * (df['Crime_Risk'] / df['Crime_Risk'].max())).astype(int)\n",
        "df['Crime_Risk'] = df['Crime_Risk'].clip(1, 10)\n",
        "df['Crime_Risk'] = np.ceil(df['Crime_Risk'] / 2).astype(int)\n",
        "\n",
        "# Normalize Crime_Risk to 0-1\n",
        "df['Crime_Risk'] = (df['Crime_Risk'] - 1) / 4\n",
        "\n",
        "# Normalize features\n",
        "features = ['Year_OCC', 'Month_OCC', 'Day_OCC', 'Hour_OCC', 'LAT', 'LON']\n",
        "scaler = StandardScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# Train-Test Split\n",
        "train_df, _ = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Dataset\n",
        "class CrimeDatasetRegression(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.x = torch.tensor(data[features].values, dtype=torch.float32)\n",
        "        self.y = torch.tensor(data['Crime_Risk'].values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "# Define Regression Model\n",
        "class CrimeRiskRegression(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(CrimeRiskRegression, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.output_layer(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# 🔁 Parallel Experiment Setup\n",
        "num_worker_settings = [0, 2, 4, 8]  # Different levels of parallelism\n",
        "batch_sizes = [128, 256, 512]  # Larger batches to reduce iterations\n",
        "hidden_dim = 256\n",
        "results = []\n",
        "\n",
        "# 🔥 Define dynamic epochs for faster training\n",
        "epoch_map = {128: 10, 256: 7, 512: 5}\n",
        "\n",
        "# Loop for Different Parallelism Configurations\n",
        "for num_workers in num_worker_settings:\n",
        "    for batch_size in batch_sizes:\n",
        "        epochs = epoch_map[batch_size]  # Dynamically set epochs\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Init model, loss, optimizer\n",
        "        model = CrimeRiskRegression(input_dim=6, hidden_dim=hidden_dim).to(device)\n",
        "        criterion = nn.HuberLoss(delta=1.0)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "        # Dataset and Loader\n",
        "        train_dataset = CrimeDatasetRegression(train_df)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=num_workers, pin_memory=True if num_workers > 0 else False\n",
        "        )\n",
        "\n",
        "        # Training Loop\n",
        "        total_loss = 0\n",
        "        sample_count = 0\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            epoch_loss = 0\n",
        "            for x_batch, y_batch in train_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(x_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item() * x_batch.size(0)\n",
        "                sample_count += x_batch.size(0)\n",
        "            total_loss = epoch_loss / sample_count\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        results.append((\n",
        "            batch_size, num_workers,\n",
        "            hidden_dim, round(elapsed_time, 3), round(total_loss, 6),\n",
        "            sample_count\n",
        "        ))\n",
        "\n",
        "# Save Results\n",
        "df_results = pd.DataFrame(results, columns=[\n",
        "    \"Batch Size\", \"Workers\", \"Hidden Dim\", \"Time (s)\",\n",
        "    \"Avg Loss (Last Epoch)\", \"Samples\"\n",
        "])\n",
        "df_results.to_csv(\"performance_results.csv\", index=False)\n",
        "\n",
        "print(\"✅ All experiments completed. Results saved to 'performance_results.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Zsm6t193TyBN",
        "outputId": "f41f6020-444a-4daf-fa6c-82215eaef5c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_02b6ef88-bc14-4dbc-bf4a-528010f9917c\", \"performance_results.csv\", 481)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"/content/performance_results.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}